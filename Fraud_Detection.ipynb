{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gulayrose/Fraud_Detection_Project/blob/main/Fraud_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jx3Ut_rjMuHR"
      },
      "source": [
        "___\n",
        "\n",
        "<p style=\"text-align: center;\"><img src=\"https://docs.google.com/uc?id=1lY0Uj5R04yMY3-ZppPWxqCr5pvBLYPnV\" class=\"img-fluid\" alt=\"CLRSWY\"></p>\n",
        "\n",
        "___"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZACetE0aJBKL",
        "outputId": "4511e796-312a-453e-c450-cc3f5371da9d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-24P_wByMuHX"
      },
      "source": [
        "# WELCOME!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ow9AD-4vMuHX"
      },
      "source": [
        "Welcome to \"***Fraud Detection Project***\". This is the last project of the Capstone Series.\n",
        "\n",
        "One of the challenges in this project is the absence of domain knowledge. So without knowing what the column names are, you will only be interested in their values. The other one is the class frequencies of the target variable are quite imbalanced.\n",
        "\n",
        "You will implement ***Logistic Regression, Random Forest, Neural Network*** algorithms and ***SMOTE*** technique. Also visualize performances of the models using ***Seaborn, Matplotlib*** and ***Yellowbrick*** in a variety of ways.\n",
        "\n",
        "At the end of the project, you will have the opportunity to deploy your model by ***Streamlit API***.\n",
        "\n",
        "Before diving into the project, please take a look at the Determines and Tasks.\n",
        "\n",
        "- ***NOTE:*** *This tutorial assumes that you already know the basics of coding in Python and are familiar with model deployement (streamlit api) as well as the theory behind Logistic Regression, Random Forest, Neural Network.*\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqbMkIZ-MuHY"
      },
      "source": [
        "---\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spCFDhO7MuHY"
      },
      "source": [
        "# #Determines\n",
        "The datasets contains transactions made by credit cards in September 2013 by european cardholders. This dataset presents transactions that occurred in two days, where it has **492 frauds** out of **284,807** transactions. The dataset is **highly unbalanced**, the positive class (frauds) account for 0.172% of all transactions.\n",
        "\n",
        "**Feature Information:**\n",
        "\n",
        "**Time**: This feature is contains the seconds elapsed between each transaction and the first transaction in the dataset. \n",
        "\n",
        "**Amount**:  This feature is the transaction Amount, can be used for example-dependant cost-senstive learning. \n",
        "\n",
        "**Class**: This feature is the target variable and it takes value 1 in case of fraud and 0 otherwise.\n",
        "\n",
        "---\n",
        "\n",
        "The aim of this project is to predict whether a credit card transaction is fraudulent. Of course, this is not easy to do.\n",
        "First of all, you need to analyze and recognize your data well in order to draw your roadmap and choose the correct arguments you will use. Accordingly, you can examine the frequency distributions of variables. You can observe variable correlations and want to explore multicollinearity. You can show the distribution of the target variable's classes over other variables. \n",
        "Also, it is useful to take missing values and outliers.\n",
        "\n",
        "After these procedures, you can move on to the model building stage by doing the basic data pre-processing you are familiar with. \n",
        "\n",
        "Start with Logistic Regression and evaluate model performance. You will apply the SMOTE technique used to increase the sample for unbalanced data. Next, rebuild your Logistic Regression model with SMOTE applied data to observe its effect.\n",
        "\n",
        "Then, you will use three different algorithms in the model building phase. You have applied Logistic Regression and Random Forest in your previous projects. However, the Deep Learning Neural Network algorithm will appear for the first time.\n",
        "\n",
        "In the final step, you will deploy your model using ***Streamlit API***. \n",
        "\n",
        "**Optional**: You can Dockerize your project and deploy on cloud."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOl6z9mXMuHY"
      },
      "source": [
        "---\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1o6X3hLLMuHZ"
      },
      "source": [
        "# #Tasks\n",
        "\n",
        "#### 1. Exploratory Data Analysis & Data Cleaning\n",
        "\n",
        "- Import Modules, Load Data & Data Review\n",
        "- Exploratory Data Analysis\n",
        "- Data Cleaning\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "#### 2. Data Preprocessing\n",
        "\n",
        "- Scaling\n",
        "- Train - Test Split\n",
        "\n",
        "\n",
        "#### 3. Model Building\n",
        "\n",
        "- Logistic Regression without SMOTE\n",
        "- Apply SMOTE\n",
        "- Logistic Regression with SMOTE\n",
        "- Random Forest Classifier with SMOTE\n",
        "- Neural Network\n",
        "\n",
        "#### 4. Model Deployement\n",
        "\n",
        "- Save and Export the Model as .pkl\n",
        "- Save and Export Variables as .pkl \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9sDSWJywMuHZ"
      },
      "source": [
        "---\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbFMU3AdMuHZ"
      },
      "source": [
        "## 1. Exploratory Data Analysis & Data Cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5nmI08_GMuHZ"
      },
      "source": [
        "### Import Modules, Load Data & Data Review"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {
        "id": "yKZtJybfMuHa"
      },
      "outputs": [],
      "source": [
        "import numpy as np \n",
        "import pandas as pd \n",
        "from pandas.plotting import register_matplotlib_converters\n",
        "%matplotlib inline\n",
        "\n",
        "import seaborn as sns\n",
        "sns.set_style(\"darkgrid\")\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "\n",
        "from pylab import rcParams\n",
        "plt.rcParams['figure.figsize'] = (6,6)\n",
        "plt.rcParams['figure.dpi'] = 100\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "warnings.warn(\"this will not show\")\n",
        "\n",
        "pd.set_option('display.max_columns', None)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unrar\n",
        "!unrar x /content/drive/MyDrive/creditcard.part1.rar"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ST-p2LEMGMg",
        "outputId": "f672e7c4-401c-4fd2-e1e0-7002f930e5b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: unrar in /usr/local/lib/python3.7/dist-packages (0.4)\n",
            "\n",
            "UNRAR 5.50 freeware      Copyright (c) 1993-2017 Alexander Roshal\n",
            "\n",
            "\n",
            "Extracting from /content/drive/MyDrive/creditcard.part1.rar\n",
            "\n",
            "\n",
            "Would you like to replace the existing file creditcard.csv\n",
            "150828752 bytes, modified on 2021-02-15 06:26\n",
            "with a new one\n",
            "150828752 bytes, modified on 2021-02-15 06:26\n",
            "\n",
            "[Y]es, [N]o, [A]ll, n[E]ver, [R]ename, [Q]uit "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/drive/MyDrive/creditcard.csv\")"
      ],
      "metadata": {
        "id": "drQCJj-gMGJp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "eq0EXGB3R3Gc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Veri kümeleri, Avrupalı ​​kart sahipleri tarafından Eylül 2013'te kredi kartlarıyla yapılan işlemleri içerir. Bu veri kümesi, 284,807 işlemden 492'sinin dolandırıcılık olduğu iki gün içinde gerçekleşen işlemleri sunar. Veri kümesi oldukça dengesizdir, pozitif sınıf (dolandırıcılık) tüm işlemlerin %0,172'sini oluşturur."
      ],
      "metadata": {
        "id": "RiE8gcbMR5Bp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "pcpP8SIZM7Cd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K22reBkbMuHa"
      },
      "source": [
        "### Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EMsdIneUnhUk"
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe().T"
      ],
      "metadata": {
        "id": "BsG7NsXyOaXb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "8V1gqvx8OaUj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.Time.value_counts() #burdaki degerler sn cinsinden "
      ],
      "metadata": {
        "id": "34x-0QPYRGaQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.Amount.value_counts()"
      ],
      "metadata": {
        "id": "k7zzQ3PCOaOU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.Class.value_counts() ##dolandırıcılık durumunda 1, aksi durumda 0 değerini alır."
      ],
      "metadata": {
        "id": "b-kKuHlrPRHr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(figsize=(6,4))\n",
        "df.Class.value_counts(ascending=False).plot.bar();\n",
        "for p in ax.patches:\n",
        "    ax.annotate((p.get_height()), (p.get_x()+0.2, p.get_height()+20),rotation=95);"
      ],
      "metadata": {
        "id": "dBQGxomFSelX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_class1=df[df.Class == 1]"
      ],
      "metadata": {
        "id": "sS5vuG5yPRES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_class0=df[df.Class== 0 ]"
      ],
      "metadata": {
        "id": "OkbdZoi2QGxK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_class1.head()"
      ],
      "metadata": {
        "id": "o4FxGGMUQOU5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[df.Class == 1].describe().T.style.background_gradient(cmap='Spectral_r')"
      ],
      "metadata": {
        "id": "FM8WVqKZcdYr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (30, 15))\n",
        "sns.heatmap(round(df.corr(), 3), annot = True, cmap = 'RdYlGn', linewidth = 0.2, annot_kws = {'size' : 16});"
      ],
      "metadata": {
        "id": "NMoQqscGcdWW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Class ile corr iliskileri yuksek olan feature' lar ile de model kurduk fakat azinlikta olan 1 class' inin corr iliskisine bakarak model kurmak daha saglikli olacagi icin feature selection islemini asagida shap yontemi ile yapmayi tercih ettik.\n"
      ],
      "metadata": {
        "id": "hm61yu2Bc7xz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15,10))\n",
        "df.corr()[\"Class\"].drop(\"Class\").sort_values().plot.barh();"
      ],
      "metadata": {
        "id": "tJmCiNblcdRp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_class0.head()"
      ],
      "metadata": {
        "id": "FZD4Lq3qQGuC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fraud islemleri 48 saat icine rastgele dagilim gostermis :\n"
      ],
      "metadata": {
        "id": "cd_rsJkJdgUd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(18,8))\n",
        "\n",
        "bins = 50\n",
        "\n",
        "ax1.hist(df.Time[df.Class == 1], bins = bins)\n",
        "ax1.set_title('Fraud')\n",
        "\n",
        "ax2.hist(df.Time[df.Class == 0], bins = bins)\n",
        "ax2.set_title('NoFraud')\n",
        "\n",
        "plt.xlabel('Time (in Seconds)')\n",
        "plt.ylabel('Number of Transactions')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Hqc_8BjHQkFM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ax = df.loc[df['Class'] == 1].plot.scatter(x='Amount', y='Class', color='Orange', label='Fraud')\n",
        "df.loc[df['Class'] == 0].plot.scatter(x='Amount', y='Class', color='Blue', label='NoFraud', ax=ax)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dw0kSYL9QkCE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ustteki ve alttaki gosterim, yapilan islemlerin fiyatini temsil ediyor. Fraud islemlerinde en fazla yapilan harcama 2500 euro civarlarinda iken Fraud olmayan islemler 25000 euro' ya kadar cikmis."
      ],
      "metadata": {
        "id": "80ew-3H5eM7r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(15,7))\n",
        "\n",
        "bins = 30\n",
        "\n",
        "ax1.hist(df.Amount[df.Class == 1], bins = bins)\n",
        "ax1.set_title('Fraud')\n",
        "\n",
        "ax2.hist(df.Amount[df.Class == 0], bins = bins)\n",
        "ax2.set_title('NoFraud')\n",
        "\n",
        "plt.xlabel('Amount ($)')\n",
        "plt.ylabel('Number of Transactions')\n",
        "plt.yscale('log')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YBKIm0xWQj-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1 ve 0 class' larinin PCA yontemi ile olusturulan componentlerin icindeki dagilimlarini gormek icin asagidaki grafikleri cizdirdik. 9, 10, 11, 12, 14, 16 ve ozellikle de 17. ve 18. componentler icinde class' larin birbirinden daha iyi ayristigini soyleyebiliriz :"
      ],
      "metadata": {
        "id": "nG40aowVedo0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.gridspec as gridspec\n"
      ],
      "metadata": {
        "id": "dJkQ79aGfNnF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(18,28*4))\n",
        "gs = gridspec.GridSpec(28, 1)\n",
        "for i, cn in enumerate(df.drop(['Time', 'Class', 'Amount'], axis=1)):\n",
        "    ax = plt.subplot(gs[i])\n",
        "    plt.hist(df[cn][df.Class == 1], bins=50, alpha = 0.7)\n",
        "    plt.hist(df[cn][df.Class == 0], bins=50, alpha = 0.3)\n",
        "    plt.yscale('log')\n",
        "    ax.set_xlabel('')\n",
        "    ax.set_title('histogram of feature: ' + str(cn))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "U_lzwV7NeXU8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.duplicated().sum()"
      ],
      "metadata": {
        "id": "U9e_0hAXeXSF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop_duplicates(inplace=True)"
      ],
      "metadata": {
        "id": "I4hNh4-VeXPd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.Class.value_counts()"
      ],
      "metadata": {
        "id": "t91F2NcrhR4m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import plotly.express as px"
      ],
      "metadata": {
        "id": "IsuQfagAhwCK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = px.pie(df, values = df['Class'].value_counts(), \n",
        "             names = (df['Class'].value_counts()).index, \n",
        "             title = '\"Class\" Column Distribution')\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "m_XZv4TzhRzQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGyEoz9fJQ0E"
      },
      "source": [
        "### Data Cleaning\n",
        "Check Missing Values and Outliers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.isna().sum().sum()"
      ],
      "metadata": {
        "id": "seYxBFqnOaRb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BvpEPuGAMuHa"
      },
      "outputs": [],
      "source": [
        "index = 0\n",
        "plt.figure(figsize=(20,20))\n",
        "for feature in df.columns :\n",
        "    if feature != 'Class' :\n",
        "        index += 1\n",
        "        plt.subplot(8,4,index)\n",
        "        sns.boxplot(x = 'Class', y = feature, data = df)\n",
        "plt.tight_layout()\n",
        "plt.show();"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature' lar icinde az da olsa outlier verilere rastlandi. Fakat feature' larimiz PCA yontemi ile elde edilen componentler oldugu icin outlier verilerin ne olduklari hakkinda bilgi sahibi degiliz. 1 class' ina ait verimiz az oldugu icin de veri kaybetmemek adina outlier verileri silmeden devam etme karari aldik :"
      ],
      "metadata": {
        "id": "pX8VOo7CV3LD"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMOO7g-sMuHb"
      },
      "source": [
        "---\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yf6VvH6WMuHb"
      },
      "source": [
        "## 2. Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlm6gCsKMuHb"
      },
      "source": [
        "#### Train - Test Split\n",
        "\n",
        "As in this case, for extremely imbalanced datasets you may want to make sure that classes are balanced across train and test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E6hc6sxsm0vQ"
      },
      "outputs": [],
      "source": [
        "X = df.drop(['Class', 'Time'], axis = 1)\n",
        "y = df.Class ##burda time da dusurudk cunku bi etlkisi olmadigi icin. datayi rahatlatmak icin . "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "metadata": {
        "id": "HeHdImpQidFR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10, stratify=y, random_state=42)"
      ],
      "metadata": {
        "id": "HVMmvckOidBY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "81xoNsUeic89"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OV28RJBeMuHb"
      },
      "source": [
        "#### Scaling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AuzpxEmKMuHb"
      },
      "outputs": [],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "operations = [(\"scaler\", StandardScaler()), ('log', LogisticRegression())]"
      ],
      "metadata": {
        "id": "LPOK4vBpizTB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HO4HAIofMuHc"
      },
      "source": [
        "---\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwQdl4PdJQ0I"
      },
      "source": [
        "## 3. Model Building\n",
        "It was previously stated that you need to make class prediction with three different algorithms. As in this case, different approaches are required to obtain better performance on unbalanced data.\n",
        "\n",
        "This dataset is severely **unbalanced** (most of the transactions are non-fraud). So the algorithms are much more likely to classify new observations to the majority class and high accuracy won't tell us anything. To address the problem of imbalanced dataset we can use undersampling and oversampling data approach techniques. Oversampling increases the number of minority class members in the training set. The advantage of oversampling is that no information from the original training set is lost unlike in undersampling, as all observations from the minority and majority classes are kept. On the other hand, it is prone to overfitting. \n",
        "\n",
        "There is a type of oversampling called **[SMOTE](https://www.geeksforgeeks.org/ml-handling-imbalanced-data-with-smote-and-near-miss-algorithm-in-python/)** (Synthetic Minority Oversampling Technique), which we are going to use to make our dataset balanced. It creates synthetic points from the minority class.\n",
        "\n",
        "- It is important that you can evaluate the effectiveness of SMOTE. For this reason, implement the Logistic Regression algorithm in two different ways, with SMOTE applied and without.\n",
        "\n",
        "***Note***: \n",
        "\n",
        "- *Do not forget to import the necessary libraries and modules before starting the model building!*\n",
        "\n",
        "- *If you are going to use the cross validation method to be more sure of the performance of your model for unbalanced data, you should make sure that the class distributions in the iterations are equal. For this case, you should use **[StratifiedKFold](https://www.analyseup.com/python-machine-learning/stratified-kfold.html)** instead of regular cross validation method.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKZcwgucJQ0I"
      },
      "source": [
        "### Logistic Regression without SMOTE\n",
        "\n",
        "- The steps you are going to cover for this algorithm are as follows: \n",
        "\n",
        "   i. Import Libraries\n",
        "   \n",
        "   *ii. Model Training*\n",
        "   \n",
        "   *iii. Prediction and Model Evaluating*\n",
        "   \n",
        "   *iv. Plot Precision and Recall Curve*\n",
        "   \n",
        "   *v. Apply and Plot StratifiedKFold*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o48s5BCdMuHd"
      },
      "source": [
        "***i. Import Libraries***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3G3cx-UjMuHd"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KD76bc5MuHd"
      },
      "source": [
        "***ii. Model Training***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g7GAK-u3MuHd"
      },
      "outputs": [],
      "source": [
        "def eval_metric(model, X_train, y_train, X_test, y_test):\n",
        "    y_train_pred = model.predict(X_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    \n",
        "    print(\"Test_Set\")\n",
        "    print(confusion_matrix(y_test, y_pred))\n",
        "    print(classification_report(y_test, y_pred))\n",
        "    print()\n",
        "    print(\"Train_Set\")\n",
        "    print(confusion_matrix(y_train, y_train_pred))\n",
        "    print(classification_report(y_train, y_train_pred))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report, precision_score, recall_score, accuracy_score, f1_score, average_precision_score"
      ],
      "metadata": {
        "id": "Vfcgnyy5jGq8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe_model = Pipeline(steps=operations)"
      ],
      "metadata": {
        "id": "jdz5eSKOjGoY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe_model.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "LmAJPclGjGmA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_metric(pipe_model, X_train, y_train, X_test, y_test)"
      ],
      "metadata": {
        "id": "mosZZoRFjGjq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CROSS VALIDATE"
      ],
      "metadata": {
        "id": "6InxhdhWjtbH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_validate"
      ],
      "metadata": {
        "id": "KK6rfQHKjGgi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "operations = [('scaler',StandardScaler()),('log',LogisticRegression())]\n",
        "model = Pipeline(operations)\n",
        "\n",
        "scores = cross_validate(model, X_train, y_train, scoring = ['precision','recall','f1','accuracy'], cv = 10)\n",
        "df_scores = pd.DataFrame(scores, index = range(1, 11))\n",
        "df_scores.mean()[2:]"
      ],
      "metadata": {
        "id": "dsbz956xj3uG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import plot_roc_curve, plot_precision_recall_curve, roc_auc_score, auc, roc_curve, average_precision_score"
      ],
      "metadata": {
        "id": "SqS5UAQIj3rf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = pipe_model.predict(X_test)\n",
        "log_f1 = f1_score(y_test, y_pred)\n",
        "log_recall = recall_score(y_test, y_pred)\n",
        "log_auc = roc_auc_score(y_test, y_pred)"
      ],
      "metadata": {
        "id": "Us457rs_j3pR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "with class_weight"
      ],
      "metadata": {
        "id": "Y5aSIKyFkqEd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class_weights = {0:1, 1:15}"
      ],
      "metadata": {
        "id": "9Jo_DRmej3mi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "operations = [(\"scaler\", StandardScaler()), ('log', LogisticRegression(class_weight=class_weights))]"
      ],
      "metadata": {
        "id": "TX_uU3Y6j3jr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe_model_weight = Pipeline(steps=operations)"
      ],
      "metadata": {
        "id": "TNWxqSmTk4My"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe_model_weight.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "92eOpNmqk4Jp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_metric(pipe_model_weight, X_train, y_train, X_test, y_test) ##class_weight isleminden sonra presicion skorlari duserken recall skorlarimiz yukseldi :"
      ],
      "metadata": {
        "id": "LH60RXX9k4GR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cross Validate sonucu elde ettigimiz precision skorlari tek seferlik skorlardan biraz dusuk cikti :"
      ],
      "metadata": {
        "id": "ms1cPR9-ldow"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "operations = [('scaler',StandardScaler()),('log',LogisticRegression(class_weight=class_weights))]\n",
        "model = Pipeline(operations)\n",
        "\n",
        "scores = cross_validate(model, X_train, y_train, scoring = ['precision','recall','f1','accuracy'], cv = 10)\n",
        "df_scores = pd.DataFrame(scores, index = range(1, 11))\n",
        "df_scores.mean()[2:]"
      ],
      "metadata": {
        "id": "8oWer2wLlhFY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvKAJVTNMuHd"
      },
      "source": [
        "***iii. Prediction and Model Evaluating***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kb68hH1TMuHd"
      },
      "outputs": [],
      "source": [
        "y_pred = pipe_model_weight.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_proba = pipe_model_weight.predict_proba(X_test)"
      ],
      "metadata": {
        "id": "x17LM7Ojl4Ac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = pd.concat([X_test, y_test], axis=1)\n",
        "test_data[\"pred\"] = y_pred\n",
        "test_data[\"pred_proba\"] = y_pred_proba[:,1]\n",
        "test_data.sample(10)"
      ],
      "metadata": {
        "id": "AIgeOFsIl39V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log_weighted_f1 = f1_score(y_test, y_pred)\n",
        "log_weighted_recall = recall_score(y_test, y_pred)\n",
        "log_weighted_auc = roc_auc_score(y_test, y_pred)"
      ],
      "metadata": {
        "id": "oJcBBH05mCrc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Datadaki bir ornekten prediction :"
      ],
      "metadata": {
        "id": "lkN0BJPLmRq1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[df.Class == 1].head()"
      ],
      "metadata": {
        "id": "J7E8I-sqmClM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe_model_weight.predict(X.loc[[541]])    # True prediction"
      ],
      "metadata": {
        "id": "XFHgpOSDmeM4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe_model_weight.predict(X.loc[[623]])      # Wrong prediction"
      ],
      "metadata": {
        "id": "eYrcsCoOmeJP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "matthews_corrcoef --> Alinan gercek degerler ile tahmin degerleri arasindaki corr\n",
        "\n",
        "matthews_corrcoef ve cohen_kappa_score dengesiz datasetlerinde genel performans icin bakilan skorlardir."
      ],
      "metadata": {
        "id": "W7xQKonFmq35"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "y_pred = pipe_model_weight.predict(X_test)\n",
        "\n",
        "matthews_corrcoef(y_test, y_pred)"
      ],
      "metadata": {
        "id": "6cK-GpjQmeGX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "cohen_kappa_score(y_test, y_pred)"
      ],
      "metadata": {
        "id": "9HinVtSdm3Kc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l193OP5fMuHd"
      },
      "source": [
        "\n",
        "You're evaluating \"accuracy score\"? Is your performance metric reflect real success? You may need to use different metrics to evaluate performance on unbalanced data. You should use **[precision and recall metrics](https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html#:~:text=The%20precision%2Drecall%20curve%20shows,a%20low%20false%20negative%20rate.)**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUDt5voIMuHe"
      },
      "source": [
        "***iv. Plot Precision and Recall Curve***\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WI0OI9SDMuHe"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_recall_curve, plot_precision_recall_curve, plot_roc_curve, roc_auc_score, roc_curve, average_precision_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot_precision_recall_curve(pipe_model_weight, X_test, y_test);"
      ],
      "metadata": {
        "id": "zy-7xzk0nFUM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_proba = pipe_model.predict_proba(X_train)\n",
        "average_precision_score(y_train, y_pred_proba[:,1])"
      ],
      "metadata": {
        "id": "fRTN_BDrnFOQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAzArHfTMuHe"
      },
      "source": [
        "***v. Apply StratifiedKFold***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ugUuOhhMuHe"
      },
      "outputs": [],
      "source": [
        "precisions, recalls, thresholds = precision_recall_curve(y_train, y_pred_proba[:,1])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimal_idx = np.argmax((2 * precisions * recalls) / (precisions + recalls))\n",
        "optimal_threshold = thresholds[optimal_idx]\n",
        "optimal_threshold"
      ],
      "metadata": {
        "id": "NzWk7PR8oN-t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedKFold    # Modeli kaç parçaya ayırmak istiyorsak ona göre index numaraları belirler.\n",
        "\n",
        "def CV(n, est, X, y, optimal_threshold):\n",
        "    skf = StratifiedKFold(n_splits = n, shuffle = True, random_state = 42)\n",
        "    acc_scores = []\n",
        "    pre_scores = []\n",
        "    rec_scores = []\n",
        "    f1_scores  = []\n",
        "    \n",
        "    X = X.reset_index(drop=True)       # Index no'ları her işlemden sonra sıfırlaması için.\n",
        "    y = y.reset_index(drop=True)\n",
        "    \n",
        "    for train_index, test_index in skf.split(X, y):\n",
        "        \n",
        "        X_train = X.loc[train_index]\n",
        "        y_train = y.loc[train_index]\n",
        "        X_test = X.loc[test_index]\n",
        "        y_test = y.loc[test_index]\n",
        "        \n",
        "        \n",
        "        est = est\n",
        "        est.fit(X_train, y_train)\n",
        "        y_pred = est.predict(X_test)\n",
        "        y_pred_proba = est.predict_proba(X_test)\n",
        "             \n",
        "        y_pred2 = pd.Series(y_pred_proba[:,1]).apply(lambda x : 1 if x >= optimal_threshold else 0)\n",
        "        \n",
        "        acc_scores.append(accuracy_score(y_test, y_pred2))\n",
        "        pre_scores.append(precision_score(y_test, y_pred2, pos_label=1))\n",
        "        rec_scores.append(recall_score(y_test, y_pred2, pos_label=1))\n",
        "        f1_scores.append(f1_score(y_test, y_pred2, pos_label=1))\n",
        "    \n",
        "    print(f'Accuracy {np.mean(acc_scores)*100:>10,.2f}%  std {np.std(acc_scores)*100:.2f}%')\n",
        "    print(f'Precision-1 {np.mean(pre_scores)*100:>7,.2f}%  std {np.std(pre_scores)*100:.2f}%')\n",
        "    print(f'Recall-1 {np.mean(rec_scores)*100:>10,.2f}%  std {np.std(rec_scores)*100:.2f}%')\n",
        "    print(f'F1_score-1 {np.mean(f1_scores)*100:>8,.2f}%  std {np.std(f1_scores)*100:.2f}%')"
      ],
      "metadata": {
        "id": "nyA773sZoN7m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CV(10, pipe_model, pd.DataFrame(X_train), y_train, 0.5)"
      ],
      "metadata": {
        "id": "lOaZymysoN4N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CV(10, pipe_model, pd.DataFrame(X_train), y_train, optimal_threshold)"
      ],
      "metadata": {
        "id": "4azaRtUyoNhj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwELs8xsJQ0Q"
      },
      "source": [
        "- Didn't the performance of the model you implemented above satisfy you? If your model is biased towards the majority class and minority class recall is not sufficient, apply **SMOTE**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4f8q5y12MuHe"
      },
      "source": [
        "### Apply SMOTE"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#conda install imblearn\n"
      ],
      "metadata": {
        "id": "TPIRL_bMLoxW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rlz070TfMuHf"
      },
      "outputs": [],
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.pipeline import Pipeline as imbpipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train.value_counts()"
      ],
      "metadata": {
        "id": "78zRkyeXpyz1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#over = SMOTE(sampling_strategy=0.1)\n",
        "#under = RandomUnderSampler(sampling_strategy=0.5)"
      ],
      "metadata": {
        "id": "jOx818PHqL2G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SMOTE islemi ile az olan 1 class' inin veri sayisini sentetik olarak artirarak yaklasik 3 katina cikardik. 0 class' ina ait veri sayisini da yaklasik 2.5 kat azalttik :"
      ],
      "metadata": {
        "id": "ERBgGIzmqN_o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "over = SMOTE(sampling_strategy={1: 1000})\n",
        "under = RandomUnderSampler(sampling_strategy={0: 100000})"
      ],
      "metadata": {
        "id": "uYrfPoufpywz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_resampled, y_resampled = over.fit_resample(X_train, y_train)"
      ],
      "metadata": {
        "id": "VkcpUR_UpyuX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_resampled, y_resampled = under.fit_resample(X_resampled, y_resampled)"
      ],
      "metadata": {
        "id": "7og8PLLIqdXB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_resampled.value_counts()"
      ],
      "metadata": {
        "id": "fVJ7vdCxqdUI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "9MgGCwoJqdRR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wvBCEvpJQ0U"
      },
      "source": [
        "### Logistic Regression with SMOTE\n",
        "\n",
        "- The steps you are going to cover for this algorithm are as follows:\n",
        "   \n",
        "   *i. Train-Test Split (Again)*\n",
        "   \n",
        "   *ii. Model Training*\n",
        "   \n",
        "   *iii. Prediction and Model Evaluating*\n",
        "   \n",
        "   *iv. Plot Precision and Recall Curve*\n",
        "   \n",
        "   *v. Apply and Plot StratifiedKFold*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJqXQ_aHMuHf"
      },
      "source": [
        "***i. Train-Test Split (Again)***\n",
        "\n",
        "Use SMOTE applied data."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, stratify=y, random_state=42)"
      ],
      "metadata": {
        "id": "R1mmUp3JK_Vs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_weight_smote = {0:1, 1:1}"
      ],
      "metadata": {
        "id": "ooU2BdWLK_Tk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "operations = [('o', over), ('u', under), (\"scaler\", StandardScaler()), \n",
        "              ('log', LogisticRegression(class_weight=class_weight_smote, random_state = 42))]"
      ],
      "metadata": {
        "id": "2tQEmwrbK_Q9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "smote_pipeline_log = imbpipeline(steps=operations)"
      ],
      "metadata": {
        "id": "-hRF2I0iK_IH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evc6DLPcMuHf"
      },
      "source": [
        "***ii. Model Training***"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "smote_pipeline_log.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "a_Gsf7UcL-qz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqJHSV5FMuHf"
      },
      "source": [
        "***iii. Prediction and Model Evaluating***"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Smote isleminden sonra kurulan Logistic Regression modelde precision ve recall skorlari birbirine biraz daha yaklasti :"
      ],
      "metadata": {
        "id": "4B_L77Q9MT5C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eval_metric(smote_pipeline_log, X_train, y_train, X_test, y_test)"
      ],
      "metadata": {
        "id": "UaQHFqygL-oK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cross Validate"
      ],
      "metadata": {
        "id": "b-6eT3R6MbhW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = smote_pipeline_log = imbpipeline(steps=operations)\n",
        "\n",
        "scores = cross_validate(model, X_train, y_train, scoring = ['accuracy', 'recall', 'f1'], cv = 10)\n",
        "df_scores = pd.DataFrame(scores, index = range(1, 11))\n",
        "df_scores.mean()[2:]\n"
      ],
      "metadata": {
        "id": "2790kvGfL-lu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "iv. Plot Precision and Recall Curve"
      ],
      "metadata": {
        "id": "AUAz-c0aMqUO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_precision_recall_curve(smote_pipeline_log, X_test, y_test);"
      ],
      "metadata": {
        "id": "6u4PpfENMl43"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HELeR2YVMuHg"
      },
      "source": [
        "***v. Apply StratifiedKFold***"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " CV(10, smote_pipeline_log, pd.DataFrame(X_train), y_train, optimal_threshold)"
      ],
      "metadata": {
        "id": "nJzMOOyJMl2A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " CV(10, smote_pipeline_log, pd.DataFrame(X_train), y_train, 0.5)"
      ],
      "metadata": {
        "id": "3lqdwHSiMlxV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = smote_pipeline_log.predict(X_test)\n",
        "smote_pipeline_f1 = f1_score(y_test, y_pred)\n",
        "smote_pipeline_recall = recall_score(y_test, y_pred)\n",
        "smote_pipeline_auc = roc_auc_score(y_test, y_pred)"
      ],
      "metadata": {
        "id": "kSDFA8EOMlvC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3zm70O7JQ0Z"
      },
      "source": [
        "### Random Forest Classifier with SMOTE\n",
        "\n",
        "- The steps you are going to cover for this algorithm are as follows:\n",
        "\n",
        "   *i. Model Training*\n",
        "   \n",
        "   *ii. Prediction and Model Evaluating*\n",
        "   \n",
        "   *iii. Plot Precision and Recall Curve*\n",
        "   \n",
        "   *iv. Apply and Plot StratifiedKFold*\n",
        "   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sr5U80HbMuHg"
      },
      "source": [
        "***i. Model Training***"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier"
      ],
      "metadata": {
        "id": "b1cMfZBeNMXM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kuvRr7f3MuHh"
      },
      "outputs": [],
      "source": [
        "class_weights = {0 : 1, 1 : 1}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "over = SMOTE(sampling_strategy={1: 1000})\n",
        "under = RandomUnderSampler(sampling_strategy={0: 100000})"
      ],
      "metadata": {
        "id": "V5q2pGNCNSjZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rf_operations = [('o', over), ('u', under), ('rf', RandomForestClassifier(class_weight=class_weights, max_depth=7, random_state=42))]\n",
        "smote_rf_model = imbpipeline(steps=rf_operations)"
      ],
      "metadata": {
        "id": "Akz6MlYjNSgh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "smote_rf_model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "MgYsQyjsNSd7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "RF default parametreler ile kurulan modelde overfit durumu gozlendi. Parametreler ile oynanarak en iyi skor max_depth=7 parametresi ile alindi :"
      ],
      "metadata": {
        "id": "Ty-5ZrFCNcTx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eval_metric(smote_rf_model, X_train, y_train, X_test, y_test)"
      ],
      "metadata": {
        "id": "jLeFgBTxNfBx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = RandomForestClassifier(class_weight = class_weights, max_depth=7, random_state=42)\n",
        "\n",
        "scores = cross_validate(model, X_train, y_train, scoring = ['accuracy', 'precision', 'recall', 'f1'], cv = 10)\n",
        "df_scores = pd.DataFrame(scores, index = range(1, 11))\n",
        "\n",
        "df_scores.mean()[2:]"
      ],
      "metadata": {
        "id": "bMJoD4OsNlv0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJ9TJdpmMuHh"
      },
      "source": [
        "***ii. Prediction and Model Evaluating***"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = smote_rf_model.predict(X_test)\n",
        "smote_rf_f1 = f1_score(y_test, y_pred)\n",
        "smote_rf_recall = recall_score(y_test, y_pred)\n",
        "smote_rf_auc = roc_auc_score(y_test, y_pred)"
      ],
      "metadata": {
        "id": "cdyH5R3rNltF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "smote_rf_model.predict(X.loc[[541]])      # True prediction\n"
      ],
      "metadata": {
        "id": "_iWBvQxcNlqp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "smote_rf_model.predict(X.loc[[623]])        # Wrong prediction"
      ],
      "metadata": {
        "id": "Lcgy39aZNloT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bdqEhrdMuHh"
      },
      "source": [
        "***iii. Plot Precision and Recall Curve***\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BaNd2jTRMuHh"
      },
      "outputs": [],
      "source": [
        "plot_precision_recall_curve(smote_rf_model, X_test, y_test);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "smne1OBWMuHh"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1n8q3JXcMuHh"
      },
      "source": [
        "***iv. Apply StratifiedKFold***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WukW9Gb3MuHh"
      },
      "outputs": [],
      "source": [
        "# without class_weight for StratifiedKFold :\n",
        "\n",
        "rf_operations = [('o', over), ('u', under), ('rf', RandomForestClassifier(max_depth=7, random_state=42))]\n",
        "rf_stratified = imbpipeline(steps=rf_operations)\n",
        "\n",
        "rf_stratified.fit(X_train, y_train)\n",
        "\n",
        "eval_metric(rf_stratified, X_train, y_train, X_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = RandomForestClassifier(max_depth=7, random_state=42)\n",
        "\n",
        "scores = cross_validate(model, X_train, y_train, scoring = ['accuracy', 'precision', 'recall', 'f1'], cv = 10)\n",
        "df_scores = pd.DataFrame(scores, index = range(1, 11))\n",
        "\n",
        "df_scores.mean()[2:]"
      ],
      "metadata": {
        "id": "pvPA6-GxSg-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_proba = rf_stratified.predict_proba(X_train)\n",
        "average_precision_score(y_train, y_pred_proba[:,1])"
      ],
      "metadata": {
        "id": "uMvK0JHjSg7r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "precisions, recalls, thresholds = precision_recall_curve(y_train, y_pred_proba[:,1])"
      ],
      "metadata": {
        "id": "2bukO5VwSg5C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimal_idx = np.argmax((2 * precisions * recalls) / (precisions + recalls))\n",
        "optimal_threshold = thresholds[optimal_idx]\n",
        "optimal_threshold"
      ],
      "metadata": {
        "id": "n9qN6BPYSg2U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CV(10, smote_rf_model, pd.DataFrame(X_train), y_train, optimal_threshold)"
      ],
      "metadata": {
        "id": "EGmXh_9YSgws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CV(10, smote_rf_model, pd.DataFrame(X_train), y_train, 0.5)\n"
      ],
      "metadata": {
        "id": "pPn27u4tS9Fh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CV(10, rf_stratified, pd.DataFrame(X_train), y_train, optimal_threshold)"
      ],
      "metadata": {
        "id": "fstbO2JES9Cm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CV(10, rf_stratified, pd.DataFrame(X_train), y_train, 0.5)"
      ],
      "metadata": {
        "id": "ekZfVhr2S8__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ife6NlFRJQ0f"
      },
      "source": [
        "### Neural Network\n",
        "\n",
        "In the final step, you will make classification with Neural Network which is a Deep Learning algorithm. \n",
        "\n",
        "Neural networks are a series of algorithms that mimic the operations of a human brain to recognize relationships between vast amounts of data. They are used in a variety of applications in financial services, from forecasting and marketing research to fraud detection and risk assessment.\n",
        "\n",
        "A neural network contains layers of interconnected nodes. Each node is a perceptron and is similar to a multiple linear regression. The perceptron feeds the signal produced by a multiple linear regression into an activation function that may be nonlinear.\n",
        "\n",
        "In a multi-layered perceptron (MLP), perceptrons are arranged in interconnected layers. The input layer collects input patterns. The output layer has classifications or output signals to which input patterns may map. \n",
        "\n",
        "Hidden layers fine-tune the input weightings until the neural network’s margin of error is minimal. It is hypothesized that hidden layers extrapolate salient features in the input data that have predictive power regarding the outputs.\n",
        "\n",
        "You will discover **[how to create](https://towardsdatascience.com/building-our-first-neural-network-in-keras-bdc8abbc17f5)** your deep learning neural network model in Python using **[Keras](https://keras.io/about/)**. Keras is a powerful and easy-to-use free open source Python library for developing and evaluating deep learning models.\n",
        "\n",
        "- The steps you are going to cover for this algorithm are as follows:\n",
        "\n",
        "   *i. Import Libraries*\n",
        "   \n",
        "   *ii. Define Model*\n",
        "    \n",
        "   *iii. Compile Model*\n",
        "   \n",
        "   *iv. Fit Model*\n",
        "   \n",
        "   *v. Prediction and Model Evaluating*\n",
        "   \n",
        "   *vi. Plot Precision and Recall Curve*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9Rl75fpMuHi"
      },
      "source": [
        "***i. Import Libraries***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LhEc3K9KMuHi"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.metrics import classification_report, confusion_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gD6Rh1R8MuHi"
      },
      "source": [
        "***ii. Define Model***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4okQmpRpMuHi"
      },
      "outputs": [],
      "source": [
        "X2 = df[feature]             \n",
        "y = df.Class.values\n",
        "seed = 42\n",
        "X_train, X_test, y_train, y_test = train_test_split(X2,\n",
        "                                                    y,\n",
        "                                                    stratify=y,\n",
        "                                                    test_size=0.1,\n",
        "                                                    random_state=seed)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()"
      ],
      "metadata": {
        "id": "tOiEqKY8fIk5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "cYJD3B23fIiS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(seed)\n",
        "\n",
        "model_ann = Sequential()\n",
        "\n",
        "model_ann.add(Dense(30, activation = \"relu\", input_dim = X_train.shape[1]))\n",
        "model_ann.add(Dense(15, activation = \"relu\"))\n",
        "model_ann.add(Dense(1, activation = \"sigmoid\"))"
      ],
      "metadata": {
        "id": "5pdxhhPLfdKd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_IQN7--qMuHi"
      },
      "source": [
        "***iii. Compile Model***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f4W96rfHMuHi"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PsM_5PhJMuHi"
      },
      "source": [
        "***iv. Fit Model***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cmkPKExFMuHj"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "InMeP9kgMuHj"
      },
      "source": [
        "***v. Prediction and Model Evaluating***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wRi_uFjIMuHj"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JAEDNkjMuHj"
      },
      "source": [
        "***vi. Plot Precision and Recall Curve***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LpbiGnpIxVK3"
      },
      "source": [
        "## 4. Model Deployement\n",
        "You cooked the food in the kitchen and moved on to the serving stage. The question is how do you showcase your work to others? Model Deployement helps you showcase your work to the world and make better decisions with it. But, deploying a model can get a little tricky at times. Before deploying the model, many things such as data storage, preprocessing, model building and monitoring need to be studied.\n",
        "\n",
        "Deployment of machine learning models, means making your models available to your other business systems. By deploying models, other systems can send data to them and get their predictions, which are in turn populated back into the company systems. Through machine learning model deployment, can begin to take full advantage of the model you built.\n",
        "\n",
        "Data science is concerned with how to build machine learning models, which algorithm is more predictive, how to design features, and what variables to use to make the models more accurate. However, how these models are actually used is often neglected. And yet this is the most important step in the machine learning pipline. Only when a model is fully integrated with the business systems, real values ​​can be extract from its predictions.\n",
        "\n",
        "After doing the following operations in this notebook, jump to a proper IDE and create your web app with Streamlit API."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCAYcMLEH_7P"
      },
      "source": [
        "### Save and Export the Model as .pkl\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MqluJ9yvIOex"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vaZP1N93IPQi"
      },
      "source": [
        "### Save and Export Variables as .pkl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q_vA-dJWxfFH"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cm9Z__Y7MuHj"
      },
      "source": [
        "___\n",
        "\n",
        "<p style=\"text-align: center;\"><img src=\"https://docs.google.com/uc?id=1lY0Uj5R04yMY3-ZppPWxqCr5pvBLYPnV\" class=\"img-fluid\" alt=\"CLRSWY\"></p>\n",
        "\n",
        "___"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Fraud Detection_Student_V2.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}